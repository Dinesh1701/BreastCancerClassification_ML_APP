a. Problem Statement

    The objective of this project is to build and compare multiple Machine Learning classification models to predict whether a breast tumor is malignant (cancerous) or benign (non-cancerous).
    
    The goal is to evaluate and compare the performance of different ML algorithms using multiple evaluation metrics and deploy the best-performing model using a Streamlit web application.

b. Dataset Description 

    The dataset used in this project is the Breast Cancer Wisconsin Diagnostic Dataset, obtained from Kaggle (UCI repository).
    
    Dataset Details:
    
    Total Instances: 569
    
    Total Features: 30 numerical features
    
    Target Variable: diagnosis
    
    M ‚Üí Malignant (1)
    
    B ‚Üí Benign (0)
    
    Data Type: Numerical
    
    Problem Type: Binary Classification
    
    Preprocessing Steps:
    
    Removed unnecessary id column
    
    Removed empty columns (if any)
    
    Converted categorical target (M/B) into numeric values (1/0)
    
    Checked and handled missing values
    
    Applied Standard Scaling before model training

c. Models Used and Evaluation Metrics 

    The following six classification models were implemented on the same dataset:
    
    ‚Ä¢	Logistic Regression
    
    ‚Ä¢	Decision Tree Classifier
    
    ‚Ä¢	K-Nearest Neighbor (kNN)
    
    ‚Ä¢	Naive Bayes (Gaussian)
    
    ‚Ä¢	Random Forest (Ensemble)
    
    ‚Ä¢	XGBoost (Ensemble)
    
    ‚Ä¢	Evaluation Metrics Used:
    
    ‚Ä¢	Accuracy
    
    ‚Ä¢	AUC Score
    
    ‚Ä¢	Precision
    
    ‚Ä¢	Recall
    
    ‚Ä¢	F1 Score
    
    ‚Ä¢	Matthews Correlation Coefficient (MCC)
    | ML Model Name       | Accuracy | AUC    | Precision | Recall | F1     | MCC    |
    | ------------------- | -------- | ------ | --------- | ------ | ------ | ------ |
    | Logistic Regression | 0.9737   | 0.9974 | 0.9762    | 0.9535 | 0.9647 | 0.9439 |
    | Decision Tree       | 0.9298   | 0.9253 | 0.9070    | 0.9070 | 0.9070 | 0.8506 |
    | kNN                 | 0.9474   | 0.9820 | 0.9302    | 0.9302 | 0.9302 | 0.8880 |
    | Naive Bayes         | 0.9649   | 0.9974 | 0.9756    | 0.9302 | 0.9524 | 0.9253 |
    | Random Forest       | 0.9649   | 0.9956 | 0.9756    | 0.9302 | 0.9524 | 0.9253 |
    | XGBoost             | 0.9561   | 0.9908 | 0.9524    | 0.9302 | 0.9412 | 0.9064 |



d. Observations on Model Performance 
    ‚Ä¢	ML Model Name	Observation about Model Performance
    ‚Ä¢	Logistic Regression	Achieved the highest accuracy and AUC score. The dataset appears nearly linearly separable, making logistic regression highly effective.
    ‚Ä¢	Decision Tree	Lower performance compared to other models. Likely slight overfitting due to tree-based structure.
    ‚Ä¢	kNN	Performed well but slightly lower than logistic regression. Sensitive to feature scaling.
    ‚Ä¢	Naive Bayes	Strong performance despite independence assumption. Worked well on numerical dataset.
    ‚Ä¢	Random Forest	Stable and robust performance due to ensemble averaging. Reduced overfitting compared to Decision Tree.
    ‚Ä¢	XGBoost	Strong boosting performance, but slightly lower than Logistic Regression for this dataset.


Streamlit Application Features

    ‚Ä¢	The application was deployed using Streamlit Community Cloud.
    
    ‚Ä¢	The Streamlit App Includes:
    
    ‚Ä¢	Dataset Upload Option (CSV format)
    
    ‚Ä¢	Model Selection Dropdown
    
    ‚Ä¢	Display of Evaluation Metrics
    
    ‚Ä¢	Confusion Matrix Output
    
    ‚Ä¢	Interactive Web Interface

üìÅ Project Structure
    project-folder/
    ‚îÇ-- app.py
    ‚îÇ-- train_models.py
    ‚îÇ-- requirements.txt
    ‚îÇ-- README.md
    ‚îÇ-- model/
        ‚îÇ-- Logistic Regression.pkl
        ‚îÇ-- Decision Tree.pkl
        ‚îÇ-- kNN.pkl
        ‚îÇ-- Naive Bayes.pkl
        ‚îÇ-- Random Forest.pkl
        ‚îÇ-- XGBoost.pkl
        ‚îÇ-- scaler.pkl

 Technologies Used

    ‚Ä¢	Python
    
    ‚Ä¢	Scikit-learn
    
    ‚Ä¢	XGBoost
    
    ‚Ä¢	Pandas
    
    ‚Ä¢	NumPy
    
    ‚Ä¢	Streamlit
    
    ‚Ä¢	Joblib

 Deployment

    The application is deployed using Streamlit Community Cloud and is accessible via the live link provided in the submission.
